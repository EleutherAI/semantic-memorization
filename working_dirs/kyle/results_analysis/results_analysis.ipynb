{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, get_dataset_split_names, DatasetDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.set_context(\"talk\")\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sample_size = None\n",
    "label_title_padding = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "memories_path = \"usvsnsp/generation-semantic-memorization-filter-results\"\n",
    "get_dataset_split_names(memories_path)\n",
    "memories_dataset = DatasetDict()\n",
    "pile_dataset = DatasetDict()\n",
    "\n",
    "splits = [split for split in get_dataset_split_names(memories_path) if \"deduped\" in split]\n",
    "for split in tqdm(splits):\n",
    "    model = split.split(\"_\")[-1]\n",
    "    if split.startswith(\"pile_\"):\n",
    "        formatted_split_name = split.replace(\"pile_\", \"\").replace(\"deduped_\", \"deduped.\")\n",
    "        pile_dataset[formatted_split_name] = load_dataset(memories_path, split=f\"{split}[:{split_sample_size}]\" if split_sample_size else split)\n",
    "    else:\n",
    "        formatted_split_name = split.replace(\"memories_\", \"\").replace(\"deduped_\", \"deduped.\")\n",
    "        memories_dataset[formatted_split_name] = load_dataset(memories_path, split=f\"{split}[:{split_sample_size}]\" if split_sample_size else split)\n",
    "\n",
    "memories_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_to_param_count = {\n",
    "    \"70m\": 70000000,\n",
    "    \"160m\": 160000000,\n",
    "    \"410m\": 410000000,\n",
    "    \"1b\": 1000000000,\n",
    "    \"1.4b\": 1400000000,\n",
    "    \"2.8b\": 2800000000,\n",
    "    \"6.9b\": 6900000000,\n",
    "    \"12b\": 12000000000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"frequencies\", \"tokens\"]\n",
    "combined_dataframe = None\n",
    "for split in tqdm(memories_dataset, desc=\"Loading Memories\"):\n",
    "    current_frame = memories_dataset[split].to_pandas()\n",
    "    current_frame.drop(columns=columns_to_drop, inplace=True)\n",
    "    current_frame[\"Model\"] = \".\".join(split.split(\".\")[1:])\n",
    "    current_frame[\"Param Count\"] = split_to_param_count[current_frame[\"Model\"].iloc[0]]\n",
    "    current_frame[\"Deduped\"] = \"deduped\" in split\n",
    "    current_frame[\"Memorized\"] = True\n",
    "    if combined_dataframe is None:\n",
    "        combined_dataframe = current_frame\n",
    "    else:\n",
    "        combined_dataframe = pd.concat([combined_dataframe, current_frame])\n",
    "\n",
    "for split in tqdm(pile_dataset, desc=\"Loading Pile\"):\n",
    "    current_frame = pile_dataset[split].to_pandas()\n",
    "    current_frame.drop(columns=columns_to_drop, inplace=True)\n",
    "    current_frame[\"Model\"] = \".\".join(split.split(\".\")[1:])\n",
    "    current_frame[\"Param Count\"] = split_to_param_count[current_frame[\"Model\"].iloc[0]]\n",
    "    current_frame[\"Deduped\"] = \"deduped\" in split\n",
    "    current_frame[\"Memorized\"] = False\n",
    "    combined_dataframe = pd.concat([combined_dataframe, current_frame])\n",
    "\n",
    "combined_dataframe = combined_dataframe.sort_values(\"Param Count\")\n",
    "display(combined_dataframe.shape)\n",
    "display(combined_dataframe.columns)\n",
    "combined_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop cases where generation_perplexity is -1\n",
    "# before_count = combined_dataframe.shape[0]\n",
    "# combined_dataframe = combined_dataframe[combined_dataframe[\"generation_perplexity\"] != -1]\n",
    "# after_count = combined_dataframe.shape[0]\n",
    "# print(f\"Dropped {before_count - after_count} rows with -1 generation_perplexity\")\n",
    "\n",
    "# set num_repeating = 0 if -1\n",
    "combined_dataframe.loc[combined_dataframe[\"num_repeating\"] == -1, \"num_repeating\"] = 0\n",
    "display(combined_dataframe.value_counts(\"num_repeating\").head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Examples to Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(row):\n",
    "    if row[\"Memorized\"] == False:\n",
    "        return \"Not Memorized\"\n",
    "    if row[\"sequence_duplicates\"] >= 200:\n",
    "        return \"Recitation\"\n",
    "    if row[\"is_incrementing\"] or row[\"num_repeating\"] != 0:\n",
    "        return \"Reconstruction\"\n",
    "\n",
    "    return \"Recollection\"\n",
    "\n",
    "combined_dataframe[\"category\"] = combined_dataframe.progress_apply(lambda row: get_category(row), axis=1)\n",
    "combined_dataframe.value_counts([\"Model\", \"Deduped\", \"category\"])\n",
    "\n",
    "# 12 deduped\n",
    "# memories_frame = combined_dataframe[(combined_dataframe[\"Memorized\"] == True) & (combined_dataframe[\"Deduped\"] == True) & (combined_dataframe[\"Model\"] == \"12b\")]\n",
    "# memories_frame[\"category\"] = memories_frame.progress_apply(lambda row: get_category(row), axis=1)\n",
    "# memories_frame.value_counts(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a 500 sample wghere Model = 12b and category = Recollection\n",
    "memories_sample = combined_dataframe[(combined_dataframe[\"Memorized\"] == True) & (combined_dataframe[\"Deduped\"] == True) & (combined_dataframe[\"Model\"] == \"12b\") & (combined_dataframe[\"category\"] == \"Recollection\")].sample(500)\n",
    "memories_sample[[\"sequence_id\", \"text\"]].to_csv(\"recollection-sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code_path = \"usvsnsp/pile-pythia-code-vs-nl-scores\"\n",
    "# code_dataset = load_dataset(code_path)[\"train\"].to_pandas()\n",
    "# code_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Join combined_dataframe with code_dataset on sequence_id\n",
    "# combined_dataframe = combined_dataframe.merge(code_dataset, on=\"sequence_id\", how=\"inner\")\n",
    "# combined_dataframe[\"is_code\"] = combined_dataframe[\"nl_score\"] <= 0.45\n",
    "# display(combined_dataframe.shape)\n",
    "# combined_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box_plot_token_stats = []\n",
    "# for param_count in tqdm(split_to_param_count.values()):\n",
    "#     model_examples = combined_dataframe[combined_dataframe[\"Param Count\"] == param_count]\n",
    "#     box_plot_token_stats.append({\n",
    "#         # \"label\": str(param_count),\n",
    "#         \"mean\": model_examples[\"avg_frequency\"].mean(),\n",
    "#         \"med\": model_examples[\"median_frequency\"].mean(),\n",
    "#         \"q1\": model_examples[\"p25_frequency\"].mean(),\n",
    "#         \"q3\": model_examples[\"p75_frequency\"].mean(),\n",
    "#         \"whislo\": model_examples[\"min_frequency\"].mean(),\n",
    "#         \"whishi\": model_examples[\"max_frequency\"].mean(),\n",
    "#     })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_plotting_frame = combined_dataframe[combined_dataframe[\"Deduped\"] == True]\n",
    "deduped_plotting_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_memories = deduped_plotting_frame[deduped_plotting_frame[\"Memorized\"] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure: Trends in Categories Over Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "counts_frame = deduped_memories.value_counts([\"Param Count\", \"category\"]).unstack().reindex(split_to_param_count.values())\n",
    "counts_frame.plot.line(\n",
    "    ax=axes[0],\n",
    "    rot=0,\n",
    "    ylabel=\"Count\",\n",
    "    marker=\"o\",\n",
    "    markersize=10,\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "# log x axis\n",
    "axes[0].set_xscale(\"log\")\n",
    "\n",
    "axes[0].set_title(\"Memorized Samples by Category\", pad=label_title_padding)\n",
    "\n",
    "# right plot is the each category across model size\n",
    "all_percents = []\n",
    "for param_count in tqdm(split_to_param_count.values()):\n",
    "    model_examples = deduped_memories[deduped_memories[\"Param Count\"] == param_count]\n",
    "    model_percents = model_examples.value_counts(\"category\", normalize=True).to_dict()\n",
    "    print(model_percents)\n",
    "    for category in model_percents:\n",
    "        all_percents.append({\n",
    "            \"Model\": model_examples[\"Model\"].unique()[0],\n",
    "            \"Param Count\": param_count,\n",
    "            \"category\": category,\n",
    "            \"percent\": model_percents[category],\n",
    "        })\n",
    "\n",
    "# create a normalized bar plot stacked by category with a seperate bar for each Model\n",
    "# have no space between bars\n",
    "percents_frame = pd.DataFrame(all_percents).pivot(index=\"Model\", columns=\"category\", values=\"percent\").reindex(split_to_param_count.keys())\n",
    "percents_frame.plot.bar(\n",
    "    stacked=True,\n",
    "    ax=axes[1],\n",
    "    rot=0,\n",
    "    width=1,\n",
    "    ylabel=\"Percent\",\n",
    ")\n",
    "\n",
    "axes[1].set_title(\"Memorized Samples by Category\", pad=label_title_padding)\n",
    "axes[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}%\".format(int(x * 100))))\n",
    "\n",
    "# remove right legend\n",
    "axes[1].get_legend().remove()\n",
    "\n",
    "# have a common legend for both plots centered below the figure. No legend box\n",
    "axes[0].legend(loc='upper center', bbox_to_anchor=(1, -0.125), ncol=4, frameon=False)\n",
    "\n",
    "# set x label for both plots as \"Parameter Count\"\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Parameter Count\")\n",
    "\n",
    "# save figure_categories_count_pcercents.png\n",
    "plt.savefig(\"figure_categories_count_percents.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "['sequence_id', 'sequence_duplicates', 'max_frequency', 'avg_frequency',\n",
    "'min_frequency', 'median_frequency', 'p25_frequency', 'p75_frequency',\n",
    "'is_incrementing', 'repeating_offset', 'num_repeating',\n",
    "'smallest_repeating_chunk', 'memorization_score',\n",
    "'templating_frequency_0.9', 'templating_frequency_0.8',\n",
    "'prompt_perplexity', 'generation_perplexity', 'sequence_perplexity',\n",
    "'Model', 'Param Count', 'Deduped', 'Memorized', 'category']\n",
    "\"\"\"\n",
    "\n",
    "titles = {\n",
    "    # Categorical\n",
    "    \"category\": \"Count of Memories by Taxonomical Category\",\n",
    "    # \"sequence_duplicates\": \"Mean Duplication Per Example\",\n",
    "    # \"is_incrementing\": \"Percent of Sequences That Are Incrementing\",\n",
    "\n",
    "    # # Length of repeating subsequences\n",
    "    # \"num_repeating\": \"Mean Token Length For Repeating Subsequences\",\n",
    "\n",
    "    # # Cosine Similarities\n",
    "    # \"templating_frequency_0.9\": \"Mean Number of Examples 0.9 Cosime Similarity To Each Example\",\n",
    "    # \"templating_frequency_0.8\": \"Mean Number of Examples 0.8 Cosime Similarity To Each Example\",\n",
    "\n",
    "    # # Perplexity\n",
    "    # \"prompt_perplexity\": \"Mean Prompt Perplexity\",\n",
    "    # \"sequence_perplexity\": \"Mean Sequence Perplexity\",\n",
    "    # \"generation_perplexity\": \"Mean Generation Perplexity\",\n",
    "\n",
    "    # # Token frequencies\n",
    "    # \"token_frequency\": \"Mean Token Frequency Statistics\",\n",
    "    # \"median_frequency\": \"Mean Median Frequency for All Unique Tokens in Each Sequence\",\n",
    "    # \"avg_frequency\": \"Mean Average Frequency for All Unique Tokens in Each Sequence\",\n",
    "    # \"p25_frequency\": \"Mean 25th Percentile Frequency for All Unique Tokens in Each Sequence\",\n",
    "    # \"min_frequency\": \"Mean Minimum Frequency for All Unique Tokens in Each Sequence\",\n",
    "\n",
    "    \"null\": \"null\"\n",
    "}\n",
    "\n",
    "# create subplots where each metric is on its own row. The first column is fo rmemorized overall and the second is broken down by category.\n",
    "fig, axes = plt.subplots(len(titles), 2, figsize=(15, 7.5 * len(titles)))\n",
    "\n",
    "for metric in tqdm(titles):\n",
    "    if metric == \"null\":\n",
    "        continue\n",
    "\n",
    "    for column in [0, 1]:\n",
    "        title_text = titles[metric]\n",
    "\n",
    "        if metric == \"token_frequency\":\n",
    "            sns.boxplot(\n",
    "                data=deduped_plotting_frame,\n",
    "                y=\"avg_frequency\",\n",
    "                x=\"Model\",\n",
    "                ax=axes[list(titles.keys()).index(metric), column],\n",
    "                gap=0.5,\n",
    "                hue=\"category\" if column == 1 else \"Memorized\",\n",
    "            )\n",
    "\n",
    "        elif metric == \"category\":\n",
    "            plotting_frame = deduped_plotting_frame[deduped_plotting_frame[\"Memorized\"] == True]\n",
    "            if column == 0:\n",
    "                sns.histplot(\n",
    "                    data=plotting_frame,\n",
    "                    x=\"Model\",\n",
    "                    hue=\"category\",\n",
    "                    ax=axes[list(titles.keys()).index(metric), column],\n",
    "                    multiple=\"stack\",\n",
    "                    stat=\"count\",\n",
    "                    common_norm=False,\n",
    "                )\n",
    "            else:\n",
    "                title_text = title_text.replace(\"Count\", \"Percent\")\n",
    "                all_percents = []\n",
    "                for param_count in tqdm(split_to_param_count.values()):\n",
    "                    model_examples = plotting_frame[plotting_frame[\"Param Count\"] == param_count]\n",
    "                    model_percents = model_examples.value_counts(\"category\", normalize=True).to_dict()\n",
    "                    for category in model_percents:\n",
    "                        all_percents.append({\n",
    "                            \"Model\": model_examples[\"Model\"].unique()[0],\n",
    "                            \"Param Count\": param_count,\n",
    "                            \"category\": category,\n",
    "                            \"percent\": model_percents[category],\n",
    "                        })\n",
    "                \n",
    "                # create a normalized bar plot stacked by category with a seperate bar for each Model\n",
    "                # have no space between bars\n",
    "                pd.DataFrame(all_percents).pivot(index=\"Model\", columns=\"category\", values=\"percent\").plot.bar(\n",
    "                    stacked=True,\n",
    "                    ax=axes[list(titles.keys()).index(metric), column],\n",
    "                    rot=0,\n",
    "                    width=1,\n",
    "                )\n",
    "                \n",
    "                axes[list(titles.keys()).index(metric), column].margins(x=0)\n",
    "\n",
    "                # make y axis percents and scale values by 100 and have %\n",
    "                axes[list(titles.keys()).index(metric), column].set_yticklabels([f\"{int(tick * 100)}%\" for tick in axes[list(titles.keys()).index(metric), column].get_yticks()])\n",
    "        else:\n",
    "            sns.lineplot(\n",
    "                data=deduped_plotting_frame.reset_index(),\n",
    "                x=\"Param Count\",\n",
    "                y=metric,\n",
    "                ax=axes[list(titles.keys()).index(metric), column],\n",
    "                markers=True,\n",
    "                hue=\"category\" if column == 1 else \"Memorized\",\n",
    "                marker=\"o\",\n",
    "            )\n",
    "\n",
    "        # log x axis if line plot\n",
    "        if metric not in [\"category\", \"token_frequency\"]:\n",
    "            axes[list(titles.keys()).index(metric), column].set_xscale(\"log\")\n",
    "\n",
    "        # set title\n",
    "        axes[list(titles.keys()).index(metric), column].set_title(title_text)\n",
    "\n",
    "        # make title bold\n",
    "        # axes[list(titles.keys()).index(metric), column].title.set_weight(\"bold\")\n",
    "\n",
    "        # set x label based off the title\n",
    "        quant_metic = title_text.split()[0]\n",
    "        axes[list(titles.keys()).index(metric), column].set_ylabel(quant_metic)\n",
    "\n",
    "        # don't use scientific notation on y axis\n",
    "        try:\n",
    "            axes[list(titles.keys()).index(metric), column].get_yaxis().get_major_formatter().set_scientific(False)\n",
    "        except:\n",
    "            print(f\"Failed to set scientific notation for {metric}\")\n",
    "\n",
    "\n",
    "# add margins between rows\n",
    "plt.subplots_adjust(hspace=0.25)\n",
    "\n",
    "# save fig\n",
    "plt.savefig(\"metrics_analysis.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recollection_sample = deduped_memories[(deduped_memories[\"Model\"] == \"12b\") & (deduped_memories[\"category\"] == \"Recollection\")].sample(500)\n",
    "recollection_sample.drop(columns=[\"Model\", \"Param Count\", \"Deduped\", \"Memorized\", \"category\"], inplace=True)\n",
    "recollection_sample.to_csv(\"recollection_sample.csv\", index=False)\n",
    "recollection_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# round \"prompt_perplexity\", \"generation_perplexity\",\t\"sequence_perplexity\" to the hundredth place\n",
    "stats_Frame = deduped_plotting_frame[(deduped_plotting_frame[\"Model\"] == \"12b\")].copy()\n",
    "stats = stats_Frame[[\"category\", \"prompt_perplexity\", \"generation_perplexity\",\t\"sequence_perplexity\"]].groupby(\"category\").describe().T\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memorization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

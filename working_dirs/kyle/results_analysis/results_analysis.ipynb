{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle/miniconda3/envs/memorization/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, get_dataset_split_names, DatasetDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set_color_codes(\"colorblind\")\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.set_context(\"talk\")\n",
    "tqdm.pandas()\n",
    "\n",
    "# set font to times new roman for plots\n",
    "sns.set_style({'font.family':'serif', 'font.serif':'Times New Roman'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sample_size = None\n",
    "label_title_padding = 10\n",
    "study_pile = False\n",
    "RECITATION_THRESHOLD = 6\n",
    "\n",
    "figures_path = f\"scale+time_figures/recitation_threshold_{RECITATION_THRESHOLD}/\"\n",
    "if not os.path.exists(figures_path):\n",
    "    os.makedirs(figures_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pile_deduped_1.4b', 'memories_deduped_410m', 'memories_deduped_1.4b', 'memories_deduped_6.9b', 'memories_deduped_1b', 'pile_deduped_410m', 'pile_deduped_2.8b', 'pile_deduped_160m', 'pile_deduped_6.9b', 'pile_deduped_1b', 'memories_deduped_70m', 'memories_deduped_2.8b', 'pile_deduped_12b', 'memories_deduped_160m', 'pile_deduped_70m', 'memories_deduped_12b', 'memories_deduped_12b.43000', 'memories_deduped_12b.103000', 'memories_deduped_12b.83000', 'memories_deduped_12b.63000', 'memories_deduped_12b.123000', 'memories_deduped_12b.23000']\n"
     ]
    }
   ],
   "source": [
    "memories_path = \"usvsnsp/generation-semantic-filters\"\n",
    "intermediate_path = \"usvsnsp/generation-semantic-intermediate-filters\"\n",
    "memories_dataset = DatasetDict()\n",
    "pile_dataset = DatasetDict()\n",
    "splits = [split for split in get_dataset_split_names(memories_path) if \"deduped\" in split] + get_dataset_split_names(intermediate_path)\n",
    "splits = [split for split in splits if \"deduped\" in split]\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Split sample size: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [05:27<00:00, 14.89s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    deduped.410m: Dataset({\n",
       "        features: ['sequence_id', 'tokens', 'text', 'is_incrementing', 'is_repeating', 'sequence_duplicates', 'max_frequency', 'avg_frequency', 'min_frequency', 'median_frequency', 'p25_frequency', 'p75_frequency', 'frequencies', 'nl_scores', '0_8_snowclones', '0_9_snowclones', '0_8_templates', '0_9_templates', 'huffman_coding_length', 'memorization_score', 'index', 'loss', 'prompt_perplexity', 'generation_perplexity', 'sequence_perplexity'],\n",
       "        num_rows: 811040\n",
       "    })\n",
       "    deduped.1.4b: Dataset({\n",
       "        features: ['sequence_id', 'tokens', 'text', 'is_incrementing', 'is_repeating', 'sequence_duplicates', 'max_frequency', 'avg_frequency', 'min_frequency', 'median_frequency', 'p25_frequency', 'p75_frequency', 'frequencies', 'nl_scores', '0_8_snowclones', '0_9_snowclones', '0_8_templates', '0_9_templates', 'huffman_coding_length', 'memorization_score', 'index', 'loss', 'prompt_perplexity', 'generation_perplexity', 'sequence_perplexity'],\n",
       "        num_rows: 1048104\n",
       "    })\n",
       "    deduped.6.9b: Dataset({\n",
       "        features: ['sequence_id', 'tokens', 'text', 'is_incrementing', 'is_repeating', 'sequence_duplicates', 'max_frequency', 'avg_frequency', 'min_frequency', 'median_frequency', 'p25_frequency', 'p75_frequency', 'frequencies', 'nl_scores', '0_8_snowclones', '0_9_snowclones', '0_8_templates', '0_9_templates', 'huffman_coding_length', 'memorization_score', 'index', 'loss', 'prompt_perplexity', 'generation_perplexity', 'sequence_perplexity'],\n",
       "        num_rows: 1680296\n",
       "    })\n",
       "    deduped.1b: Dataset({\n",
       "        features: ['sequence_id', 'tokens', 'text', 'is_incrementing', 'is_repeating', 'sequence_duplicates', 'max_frequency', 'avg_frequency', 'min_frequency', 'median_frequency', 'p25_frequency', 'p75_frequency', 'frequencies', 'nl_scores', '0_8_snowclones', '0_9_snowclones', '0_8_templates', '0_9_templates', 'huffman_coding_length', 'memorization_score', 'index', 'loss', 'prompt_perplexity', 'generation_perplexity', 'sequence_perplexity'],\n",
       "        num_rows: 1032872\n",
       "    })\n",
       "    deduped.70m: Dataset({\n",
       "        features: ['sequence_id', 'tokens', 'text', 'is_incrementing', 'is_repeating', 'sequence_duplicates', 'max_frequency', 'avg_frequency', 'min_frequency', 'median_frequency', 'p25_frequency', 'p75_frequency', 'frequencies', 'nl_scores', '0_8_snowclones', '0_9_snowclones', '0_8_templates', '0_9_templates', 'huffman_coding_length', 'memorization_score', 'index', 'loss', 'prompt_perplexity', 'generation_perplexity', 'sequence_perplexity'],\n",
       "        num_rows: 411448\n",
       "    })\n",
       "    deduped.2.8b: Dataset({\n",
       "        features: ['sequence_id', 'tokens', 'text', 'is_incrementing', 'is_repeating', 'sequence_duplicates', 'max_frequency', 'avg_frequency', 'min_frequency', 'median_frequency', 'p25_frequency', 'p75_frequency', 'frequencies', 'nl_scores', '0_8_snowclones', '0_9_snowclones', '0_8_templates', '0_9_templates', 'huffman_coding_length', 'memorization_score', 'index', 'loss', 'prompt_perplexity', 'generation_perplexity', 'sequence_perplexity'],\n",
       "        num_rows: 1355216\n",
       "    })\n",
       "    deduped.160m: Dataset({\n",
       "        features: ['sequence_id', 'tokens', 'text', 'is_incrementing', 'is_repeating', 'sequence_duplicates', 'max_frequency', 'avg_frequency', 'min_frequency', 'median_frequency', 'p25_frequency', 'p75_frequency', 'frequencies', 'nl_scores', '0_8_snowclones', '0_9_snowclones', '0_8_templates', '0_9_templates', 'huffman_coding_length', 'memorization_score', 'index', 'loss', 'prompt_perplexity', 'generation_perplexity', 'sequence_perplexity'],\n",
       "        num_rows: 581200\n",
       "    })\n",
       "    deduped.12b: Dataset({\n",
       "        features: ['sequence_id', 'tokens', 'text', 'is_incrementing', 'is_repeating', 'sequence_duplicates', 'max_frequency', 'avg_frequency', 'min_frequency', 'median_frequency', 'p25_frequency', 'p75_frequency', 'frequencies', 'nl_scores', '0_8_snowclones', '0_9_snowclones', '0_8_templates', '0_9_templates', 'huffman_coding_length', 'memorization_score', 'index', 'loss', 'prompt_perplexity', 'generation_perplexity', 'sequence_perplexity'],\n",
       "        num_rows: 1871216\n",
       "    })\n",
       "    deduped.12b.43000: Dataset({\n",
       "        features: ['sequence_id', 'tokens', 'text', 'is_incrementing', 'is_repeating', 'sequence_duplicates', 'max_frequency', 'avg_frequency', 'min_frequency', 'median_frequency', 'p25_frequency', 'p75_frequency', 'frequencies', 'nl_scores', 'huffman_coding_length', 'memorization_score'],\n",
       "        num_rows: 358863\n",
       "    })\n",
       "    deduped.12b.103000: Dataset({\n",
       "        features: ['sequence_id', 'tokens', 'text', 'is_incrementing', 'is_repeating', 'sequence_duplicates', 'max_frequency', 'avg_frequency', 'min_frequency', 'median_frequency', 'p25_frequency', 'p75_frequency', 'frequencies', 'nl_scores', 'huffman_coding_length', 'memorization_score'],\n",
       "        num_rows: 1195578\n",
       "    })\n",
       "    deduped.12b.83000: Dataset({\n",
       "        features: ['sequence_id', 'tokens', 'text', 'is_incrementing', 'is_repeating', 'sequence_duplicates', 'max_frequency', 'avg_frequency', 'min_frequency', 'median_frequency', 'p25_frequency', 'p75_frequency', 'frequencies', 'nl_scores', 'huffman_coding_length', 'memorization_score'],\n",
       "        num_rows: 852068\n",
       "    })\n",
       "    deduped.12b.63000: Dataset({\n",
       "        features: ['sequence_id', 'tokens', 'text', 'is_incrementing', 'is_repeating', 'sequence_duplicates', 'max_frequency', 'avg_frequency', 'min_frequency', 'median_frequency', 'p25_frequency', 'p75_frequency', 'frequencies', 'nl_scores', 'huffman_coding_length', 'memorization_score'],\n",
       "        num_rows: 585067\n",
       "    })\n",
       "    deduped.12b.123000: Dataset({\n",
       "        features: ['sequence_id', 'tokens', 'text', 'is_incrementing', 'is_repeating', 'sequence_duplicates', 'max_frequency', 'avg_frequency', 'min_frequency', 'median_frequency', 'p25_frequency', 'p75_frequency', 'frequencies', 'nl_scores', 'huffman_coding_length', 'memorization_score'],\n",
       "        num_rows: 1564055\n",
       "    })\n",
       "    deduped.12b.23000: Dataset({\n",
       "        features: ['sequence_id', 'tokens', 'text', 'is_incrementing', 'is_repeating', 'sequence_duplicates', 'max_frequency', 'avg_frequency', 'min_frequency', 'median_frequency', 'p25_frequency', 'p75_frequency', 'frequencies', 'nl_scores', 'huffman_coding_length', 'memorization_score'],\n",
       "        num_rows: 163418\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    \n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Loading datasets...\")\n",
    "print(f\"Split sample size: {split_sample_size}\")\n",
    "\n",
    "for split in tqdm(splits):\n",
    "    model = split.split(\"_\")[-1]\n",
    "    checkpoint = int(split.split(\".\")[-1]) if split.split(\".\")[-1][1].isnumeric() else 143000\n",
    "    formatted_split_name = split.replace(\"memories_\", \"\").replace(\"deduped_\", \"deduped.\").replace(\"pile_\", \"\")\n",
    "    dataset_path = memories_path if checkpoint == 143000 else intermediate_path\n",
    "    if \"memories\" in split: \n",
    "        # continue # Don't load memories since they're unnecessary for the first part of the analysis\n",
    "        memories_dataset[formatted_split_name] = load_dataset(dataset_path, split=f\"{split}[:{split_sample_size}]\" if split_sample_size else split)\n",
    "    else:\n",
    "        continue\n",
    "        pile_dataset[formatted_split_name] = load_dataset(dataset_path, split=f\"{split}[:{split_sample_size}]\" if split_sample_size else split)\n",
    "\n",
    "display(memories_dataset)\n",
    "display(pile_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_to_param_count = {\n",
    "    \"70m\": 70000000,\n",
    "    \"410m\": 410000000,\n",
    "    \"1b\": 1000000000,\n",
    "    \"1.4b\": 1400000000,\n",
    "    \"2.8b\": 2800000000,\n",
    "    \"6.9b\": 6900000000,\n",
    "    \"12b\": 12000000000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Memories:  43%|████▎     | 6/14 [00:04<00:05,  1.39it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'160m'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m combined_dataframe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m tqdm(memories_dataset, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading Memories\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     current_frame \u001b[38;5;241m=\u001b[39m \u001b[43mget_frame_from_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemories_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m combined_dataframe \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m         combined_dataframe \u001b[38;5;241m=\u001b[39m current_frame\n",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m, in \u001b[0;36mget_frame_from_split\u001b[0;34m(dataset, split_name, is_pile_sample)\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m split\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeduped\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m checkpoint \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m split\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m      8\u001b[0m current_frame[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m----> 9\u001b[0m current_frame[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParam Count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msplit_to_param_count\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_frame\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mModel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m current_frame[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeduped\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeduped\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m split\n\u001b[1;32m     11\u001b[0m current_frame[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemorized\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m current_frame[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemorization_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: '160m'"
     ]
    }
   ],
   "source": [
    "def get_frame_from_split(dataset, split_name, is_pile_sample):\n",
    "    current_frame = dataset[split].to_pandas()\n",
    "    current_frame.drop(columns=columns_to_drop, inplace=True)\n",
    "    checkpoint = int(split.split(\".\")[-1]) if split.split(\".\")[-1][1].isnumeric() and len(split.split(\".\")) != 2 else \"Final\"\n",
    "    current_frame[\"Checkpoint\"] = checkpoint\n",
    "    current_frame[\"TrainingPercentage\"] = 1 if checkpoint == \"Final\" else checkpoint / 143000\n",
    "    model = split.split(\"deduped\")[-1][1:] if checkpoint == \"Final\" else split.split(\".\")[-2]\n",
    "    current_frame[\"Model\"] = model\n",
    "    current_frame[\"Param Count\"] = split_to_param_count[current_frame[\"Model\"].iloc[0]]\n",
    "    current_frame[\"Deduped\"] = \"deduped\" in split\n",
    "    current_frame[\"Memorized\"] = current_frame[\"memorization_score\"] >= 1\n",
    "    current_frame[\"IsPileSample\"] = is_pile_sample\n",
    "    return current_frame\n",
    "\n",
    "\n",
    "columns_to_drop = [\"frequencies\", \"tokens\", \"text\"]\n",
    "combined_dataframe = None\n",
    "for split in tqdm(memories_dataset, desc=\"Loading Memories\"):\n",
    "    current_frame = get_frame_from_split(memories_dataset, split, False)\n",
    "    if combined_dataframe is None:\n",
    "        combined_dataframe = current_frame\n",
    "    else:\n",
    "        combined_dataframe = pd.concat([combined_dataframe, current_frame])\n",
    "\n",
    "for split in tqdm(pile_dataset, desc=\"Loading Pile\"):\n",
    "    current_frame = get_frame_from_split(pile_dataset, split, True)\n",
    "    combined_dataframe = pd.concat([combined_dataframe, current_frame])\n",
    "\n",
    "combined_dataframe = combined_dataframe.sort_values(\"Param Count\")\n",
    "# display(combined_dataframe.shape)\n",
    "# display(combined_dataframe.columns)\n",
    "combined_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataframe = combined_dataframe[combined_dataframe[\"Model\"] != \"160m\"]\n",
    "combined_dataframe.value_counts(\"Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Examples to Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(row):\n",
    "    if row[\"Memorized\"] == False:\n",
    "        return \"Not Memorized\"\n",
    "    if row[\"sequence_duplicates\"] >= RECITATION_THRESHOLD:\n",
    "        return \"Recitation\"\n",
    "    if row[\"is_incrementing\"] or row[\"is_repeating\"]:\n",
    "        return \"Reconstruction\"\n",
    "\n",
    "    return \"Recollection\"\n",
    "\n",
    "combined_dataframe[\"category\"] = combined_dataframe.progress_apply(lambda row: get_category(row), axis=1)\n",
    "\n",
    "combined_dataframe.value_counts([\"Model\", \"Checkpoint\", \"category\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Count and Memories by Taxonomy Across Time and Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_memories_dataframe = combined_dataframe[combined_dataframe[\"IsPileSample\"] == False]\n",
    "combined_memories_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot for data across \n",
    "counts_frame_scale = combined_memories_dataframe[combined_memories_dataframe[\"Checkpoint\"] == \"Final\"].value_counts([\"Param Count\", \"category\"]).unstack().reindex(split_to_param_count.values())\n",
    "counts_frame_scale.to_csv(f\"final_checkpoint_counts_recitation={RECITATION_THRESHOLD}.csv\")\n",
    "display(counts_frame_scale)\n",
    "\n",
    "intermediate_frame = combined_memories_dataframe[combined_memories_dataframe[\"Model\"] == \"12b\"]\n",
    "sorted_checkpoints = sorted(sorted(intermediate_frame[\"TrainingPercentage\"].unique(), key=lambda x: int(x)))\n",
    "\n",
    "counts_frame_time = intermediate_frame.value_counts([\"TrainingPercentage\", \"category\"]).unstack().reindex(sorted_checkpoints)\n",
    "counts_frame_time.to_csv(f\"intermediate_checkpoint_counts_recitation={RECITATION_THRESHOLD}.csv\")\n",
    "display(counts_frame_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 3))\n",
    "\n",
    "# set figure 1\n",
    "# first figure is a line plot of the counts of each category for across intermediate checkpoints\n",
    "\n",
    "sns.lineplot(ax=axes[0], data=counts_frame_time, dashes=False, markers=True, markersize=8)\n",
    "\n",
    "# rotate x axis labels\n",
    "# axes[0].tick_params(axis='x', rotation=20)\n",
    "\n",
    "# Set x axix label to \"Training Time\"\n",
    "axes[0].set_xlabel(\"Training Time\")\n",
    "\n",
    "# make x labels smaller\n",
    "axes[0].tick_params(axis='x', labelsize=12)\n",
    "\n",
    "# the x axis values are between 0 and 1. Set the x axis to be a percentage\n",
    "axes[0].xaxis.set_major_formatter(PercentFormatter(1))\n",
    "\n",
    "# Add y label for Count\n",
    "axes[0].set_ylabel(\"Count\", labelpad=label_title_padding)\n",
    "\n",
    "# add padding for labels\n",
    "fig.subplots_adjust(wspace=0.2)\n",
    "\n",
    "# make x and y axis log scale\n",
    "axes[0].set_yscale(\"log\")\n",
    "\n",
    "# # have a common legend for both plots centered below the figure. No legend box\n",
    "axes[0].legend(loc='upper center', bbox_to_anchor=(1.1, -0.3), ncol=4, frameon=False)\n",
    "\n",
    "# set figure 2\n",
    "# the second figure is a line plot of the counts of each category across parameter count\n",
    "\n",
    "sns.lineplot(ax=axes[1], data=counts_frame_scale, dashes=False, markers=True, markersize=8)\n",
    "\n",
    "# make x and y axis log scale\n",
    "axes[1].set_xscale(\"log\")\n",
    "axes[1].set_yscale(\"log\")\n",
    "\n",
    "# Set x label to \"Parameters\"\n",
    "axes[1].set_xlabel(\"Parameters\")\n",
    "\n",
    "# Add y label for Count\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "# remove legend\n",
    "axes[1].legend().remove()\n",
    "\n",
    "# add padding for labels\n",
    "fig.subplots_adjust(wspace=0.30)\n",
    "\n",
    "fig.savefig(f\"{figures_path}/categories_counts_across_time+scale.pdf\", bbox_inches=\"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Percents and Memories by Taxonomy Across Time and Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_frame = combined_memories_dataframe.value_counts([\"Param Count\", \"category\"]).unstack().reindex(split_to_param_count.values())\n",
    "display(counts_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_percents_time = []\n",
    "for checkpoint in tqdm(sorted_checkpoints):\n",
    "    model_examples = combined_memories_dataframe[combined_memories_dataframe[\"TrainingPercentage\"] == checkpoint]\n",
    "    model_percents = model_examples.value_counts(\"category\", normalize=True).to_dict()\n",
    "    for category in model_percents:\n",
    "        all_percents_time.append({\n",
    "            \"TrainingPercentage\": checkpoint,\n",
    "            \"category\": category,\n",
    "            \"percent\": model_percents[category],\n",
    "        })\n",
    "\n",
    "percents_frame_time = pd.DataFrame(all_percents_time).pivot(index=\"TrainingPercentage\", columns=\"category\", values=\"percent\").reindex(sorted_checkpoints)\n",
    "percents_frame_time.index = [f\"{int(percent * 100)}%\" for percent in percents_frame_time.index]\n",
    "percents_frame_time.to_csv(f\"percents_frame_time_recitation={RECITATION_THRESHOLD}.csv\")\n",
    "display(percents_frame_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_percents_scale = []\n",
    "for param_count in tqdm(split_to_param_count.values()):\n",
    "    model_examples = combined_memories_dataframe[combined_memories_dataframe[\"Param Count\"] == param_count]\n",
    "    model_percents = model_examples.value_counts(\"category\", normalize=True).to_dict()\n",
    "    for category in model_percents:\n",
    "        all_percents_scale.append({\n",
    "            \"Model\": model_examples[\"Model\"].unique()[0],\n",
    "            \"Param Count\": param_count,\n",
    "            \"category\": category,\n",
    "            \"percent\": model_percents[category],\n",
    "        })\n",
    "\n",
    "percents_frame_scale = pd.DataFrame(all_percents_scale).pivot(index=\"Model\", columns=\"category\", values=\"percent\").reindex(split_to_param_count.keys())\n",
    "percents_frame_scale.to_csv(f\"percents_frame_scale_recitation={RECITATION_THRESHOLD}.csv\")\n",
    "display(percents_frame_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(13, 3))\n",
    "plots = [\n",
    "    (percents_frame_time, axes[0]),\n",
    "    (percents_frame_scale, axes[1])\n",
    "]\n",
    "\n",
    "for idx, (data, ax) in enumerate(plots):\n",
    "    data.plot.bar(\n",
    "        stacked=True,\n",
    "        ax=ax,\n",
    "        rot=0,\n",
    "        width=1,\n",
    "        ylabel=\"Memories\",\n",
    "    )\n",
    "\n",
    "    if idx == 0:\n",
    "        ax.set_xlabel(\"Training Time\")\n",
    "    else:\n",
    "        ax.set_xlabel(\"Parameter Count\")\n",
    "    \n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}%\".format(int(x * 100))))\n",
    "    ax.tick_params(axis='x', rotation=20, labelsize=12)\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "# Adjusting subplot parameters\n",
    "fig.subplots_adjust(wspace=0.30)\n",
    "\n",
    "# Common legend configuration\n",
    "axes[0].legend(loc='upper center', bbox_to_anchor=(1.1, -0.4), ncol=4, frameon=False)\n",
    "\n",
    "# align x axis labels\n",
    "fig.align_xlabels()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig(f\"{figures_path}/categories_percents_across_time+scale.pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure: Combined Counts + Percents Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single row with four plots. Plot ordering is counts across scale, percents across scale, counts across time, percents across time\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 3))\n",
    "\n",
    "# set figure 1\n",
    "sns.lineplot(ax=axes[0], data=counts_frame_scale, dashes=False, markers=True, markersize=8)\n",
    "axes[0].tick_params(axis='x', rotation=20, labelsize=10)\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].set_xscale(\"log\")\n",
    "axes[0].tick_params(axis='y', labelsize=10)\n",
    "axes[0].set_ylabel(\"Memories\")\n",
    "axes[0].set_xlabel(\"Parameters\")\n",
    "axes[0].legend(loc='upper center', bbox_to_anchor=(2.25, -0.3), ncol=4, frameon=False)\n",
    "\n",
    "# set figure 2\n",
    "percents_frame_scale.plot.bar(\n",
    "    stacked=True,\n",
    "    ax=axes[1],\n",
    "    rot=0,\n",
    "    width=1,\n",
    ")\n",
    "axes[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}%\".format(int(x * 100))))\n",
    "axes[1].tick_params(axis='x', rotation=20, labelsize=10)\n",
    "axes[1].tick_params(axis='y', labelsize=10)\n",
    "axes[1].set_yticks([0.2, 0.6, 1])\n",
    "axes[1].set_xlabel(\"Parameters\")\n",
    "axes[1].get_legend().remove()\n",
    "\n",
    "# set figure 3\n",
    "sns.lineplot(ax=axes[2], data=counts_frame_time, dashes=False, markers=True, markersize=8)\n",
    "axes[2].set_yscale(\"log\")\n",
    "axes[2].tick_params(axis='x', labelsize=10)\n",
    "axes[2].tick_params(axis='y', labelsize=10)\n",
    "#  log x\n",
    "axes[2].set_xscale(\"log\", base=2)\n",
    "# set x ticks at 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\n",
    "axes[2].set_xticks(sorted_checkpoints)\n",
    "# axes[2].set_xticks([0.16, 0.2, 0.4, 0.8, 1])\n",
    "# rotate\n",
    "axes[2].tick_params(axis='x', rotation=30)\n",
    "axes[2].set_xlabel(\"Training Time\")\n",
    "axes[2].xaxis.set_major_formatter(PercentFormatter(1))\n",
    "axes[2].legend().remove()\n",
    "\n",
    "# set figure 4\n",
    "percents_frame_time.plot.bar(\n",
    "    stacked=True,\n",
    "    ax=axes[3],\n",
    "    rot=0,\n",
    "    width=1,\n",
    ")\n",
    "axes[3].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}%\".format(int(x * 100))))\n",
    "axes[3].tick_params(axis='x', rotation=20, labelsize=10)\n",
    "axes[3].tick_params(axis='y', labelsize=10)\n",
    "axes[3].set_yticks([0.2, 0.6, 1])\n",
    "axes[3].set_xlabel(\"Training Time\")\n",
    "axes[3].get_legend().remove()\n",
    "\n",
    "# make all the x labels have the same height\n",
    "fig.align_xlabels()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig(f\"{figures_path}/categories_counts_percents_across_time+scale.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same chart but with hugher bar charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single row with four plots. Plot ordering is counts across scale, percents across scale, counts across time, percents across time\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 3))\n",
    "\n",
    "# set figure 1\n",
    "sns.lineplot(ax=axes[0], data=counts_frame_scale, dashes=False, markers=True, markersize=8)\n",
    "axes[0].tick_params(axis='x', rotation=20, labelsize=10)\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].set_xscale(\"log\")\n",
    "axes[0].tick_params(axis='y', labelsize=10)\n",
    "axes[0].set_ylabel(\"Memories\")\n",
    "axes[0].set_xlabel(\"Parameters\")\n",
    "axes[0].legend(loc='upper center', bbox_to_anchor=(2.25, -0.3), ncol=4, frameon=False)\n",
    "\n",
    "# set figure 2\n",
    "percents_frame_scale.plot.bar(\n",
    "    stacked=True,\n",
    "    ax=axes[1],\n",
    "    rot=0,\n",
    "    width=1,\n",
    ")\n",
    "axes[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}%\".format(int(x * 100))))\n",
    "axes[1].set_ylim(0.8, 1)\n",
    "axes[1].set_yticks([0.8, 0.85, 0.9, 0.95, 1])\n",
    "axes[1].tick_params(axis='x', rotation=20, labelsize=10)\n",
    "axes[1].tick_params(axis='y', labelsize=10)\n",
    "# axes[1].set_yticks([0.2, 0.6, 1])\n",
    "axes[1].set_xlabel(\"Parameters\")\n",
    "axes[1].get_legend().remove()\n",
    "\n",
    "# set figure 3\n",
    "sns.lineplot(ax=axes[2], data=counts_frame_time, dashes=False, markers=True, markersize=8)\n",
    "axes[2].set_yscale(\"log\")\n",
    "axes[2].tick_params(axis='x', labelsize=10)\n",
    "axes[2].tick_params(axis='y', labelsize=10)\n",
    "#  log x\n",
    "axes[2].set_xscale(\"log\", base=2)\n",
    "# set x ticks at 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\n",
    "axes[2].set_xticks(sorted_checkpoints)\n",
    "# axes[2].set_xticks([0.16, 0.2, 0.4, 0.8, 1])\n",
    "# rotate\n",
    "axes[2].tick_params(axis='x', rotation=30)\n",
    "axes[2].set_xlabel(\"Training Time\")\n",
    "axes[2].xaxis.set_major_formatter(PercentFormatter(1))\n",
    "axes[2].legend().remove()\n",
    "\n",
    "# set figure 4\n",
    "percents_frame_time.plot.bar(\n",
    "    stacked=True,\n",
    "    ax=axes[3],\n",
    "    rot=0,\n",
    "    width=1,\n",
    ")\n",
    "axes[3].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}%\".format(int(x * 100))))\n",
    "axes[3].set_ylim(0.8, 1)\n",
    "axes[3].set_yticks([0.8, 0.85, 0.9, 0.95, 1])\n",
    "axes[3].tick_params(axis='x', rotation=20, labelsize=10)\n",
    "axes[3].tick_params(axis='y', labelsize=10)\n",
    "axes[3].set_xlabel(\"Training Time\")\n",
    "axes[3].get_legend().remove()\n",
    "\n",
    "# make all the x labels have the same height\n",
    "fig.align_xlabels()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig(f\"{figures_path}/categories_counts_percents_across_time+scale_bounded_bars.pdf\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memorization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

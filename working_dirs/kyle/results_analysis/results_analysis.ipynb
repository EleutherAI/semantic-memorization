{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle/miniconda3/envs/memorization/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter, MaxNLocator, LogLocator \n",
    "from matplotlib import transforms\n",
    "from datasets import load_dataset, get_dataset_split_names, DatasetDict\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "sns.set_color_codes(\"colorblind\")\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.set_context(\"talk\")\n",
    "# sns.color_palette()\n",
    "sns.color_palette(\"tab10\")\n",
    "\n",
    "# set font to times new roman for plots\n",
    "sns.set_style({'font.family':'serif', 'font.serif':'Times New Roman'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sample_size = None\n",
    "label_title_padding = 10\n",
    "study_pile = False\n",
    "RECITATION_THRESHOLD = 5\n",
    "\n",
    "figures_path = f\"scale+time_figures/recitation_threshold_{RECITATION_THRESHOLD}/\"\n",
    "if not os.path.exists(figures_path):\n",
    "    os.makedirs(figures_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "memories_path = \"usvsnsp/generation-semantic-filters\"\n",
    "intermediate_path = \"usvsnsp/generation-semantic-intermediate-filters\"\n",
    "memories_dataset = DatasetDict()\n",
    "pile_dataset = DatasetDict()\n",
    "splits = [split for split in get_dataset_split_names(memories_path) if \"deduped\" in split] + get_dataset_split_names(intermediate_path)\n",
    "splits = [split for split in splits if \"deduped\" in split]\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading datasets...\")\n",
    "print(f\"Split sample size: {split_sample_size}\")\n",
    "\n",
    "for split in tqdm(splits):\n",
    "    model = split.split(\"_\")[-1]\n",
    "    checkpoint = int(split.split(\".\")[-1]) if split.split(\".\")[-1][1].isnumeric() else 143000\n",
    "    formatted_split_name = split.replace(\"memories_\", \"\").replace(\"deduped_\", \"deduped.\").replace(\"pile_\", \"\")\n",
    "    dataset_path = memories_path if checkpoint == 143000 else intermediate_path\n",
    "    if \"memories\" in split: \n",
    "        # continue # Don't load memories since they're unnecessary for the first part of the analysis\n",
    "        memories_dataset[formatted_split_name] = load_dataset(dataset_path, split=f\"{split}[:{split_sample_size}]\" if split_sample_size else split)\n",
    "    else:\n",
    "        # continue\n",
    "        pile_dataset[formatted_split_name] = load_dataset(dataset_path, split=f\"{split}[:{split_sample_size}]\" if split_sample_size else split)\n",
    "\n",
    "display(memories_dataset)\n",
    "display(pile_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_to_param_count = {\n",
    "    \"70m\": 70000000,\n",
    "    \"410m\": 410000000,\n",
    "    \"160m\": 160000000,\n",
    "    \"1b\": 1000000000,\n",
    "    \"1.4b\": 1400000000,\n",
    "    \"2.8b\": 2800000000,\n",
    "    \"6.9b\": 6900000000,\n",
    "    \"12b\": 12000000000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_from_split(dataset, split_name, is_pile_sample):\n",
    "    current_frame = dataset[split].to_pandas()\n",
    "    current_frame.drop(columns=columns_to_drop, inplace=True)\n",
    "    checkpoint = int(split.split(\".\")[-1]) if split.split(\".\")[-1][1].isnumeric() and len(split.split(\".\")) != 2 else \"Final\"\n",
    "    current_frame[\"Checkpoint\"] = checkpoint\n",
    "    current_frame[\"TrainingPercentage\"] = 1 if checkpoint == \"Final\" else checkpoint / 143000\n",
    "    model = split.split(\"deduped\")[-1][1:] if checkpoint == \"Final\" else split.split(\".\")[-2]\n",
    "    current_frame[\"Model\"] = model\n",
    "    current_frame[\"Param Count\"] = split_to_param_count[current_frame[\"Model\"].iloc[0]]\n",
    "    current_frame[\"Deduped\"] = \"deduped\" in split\n",
    "    current_frame[\"Memorized\"] = current_frame[\"memorization_score\"] >= 1\n",
    "    current_frame[\"IsPileSample\"] = is_pile_sample\n",
    "    current_frame[\"IsCode\"] = current_frame[\"nl_scores\"].apply(lambda x: x <= 0.45)\n",
    "    return current_frame\n",
    "\n",
    "\n",
    "columns_to_drop = [\"frequencies\", \"tokens\", \"text\"]\n",
    "combined_dataframe = None\n",
    "for split in tqdm(memories_dataset, desc=\"Loading Memories\"):\n",
    "    current_frame = get_frame_from_split(memories_dataset, split, False)\n",
    "    if combined_dataframe is None:\n",
    "        combined_dataframe = current_frame\n",
    "    else:\n",
    "        combined_dataframe = pd.concat([combined_dataframe, current_frame])\n",
    "\n",
    "for split in tqdm(pile_dataset, desc=\"Loading Pile\"):\n",
    "    current_frame = get_frame_from_split(pile_dataset, split, True)\n",
    "    combined_dataframe = pd.concat([combined_dataframe, current_frame])\n",
    "\n",
    "combined_dataframe = combined_dataframe.sort_values(\"Param Count\")\n",
    "combined_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataframe = combined_dataframe[combined_dataframe[\"Model\"] != \"160m\"]\n",
    "combined_dataframe.value_counts(\"Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Examples to Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(row):\n",
    "    if row[\"Memorized\"] == False:\n",
    "        return \"Not Memorized\"\n",
    "    if row[\"sequence_duplicates\"] > RECITATION_THRESHOLD:\n",
    "        return \"Recitation\"\n",
    "    if row[\"is_incrementing\"] or row[\"is_repeating\"]:\n",
    "        return \"Reconstruction\"\n",
    "\n",
    "    return \"Recollection\"\n",
    "\n",
    "combined_dataframe[\"category\"] = combined_dataframe.progress_apply(lambda row: get_category(row), axis=1)\n",
    "combined_dataframe.value_counts([\"Model\", \"Checkpoint\", \"category\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# harcoded order\n",
    "categories = [\"Recitation\", \"Reconstruction\", \"Recollection\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Count and Memories by Taxonomy Across Time and Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_memories_dataframe = combined_dataframe[combined_dataframe[\"IsPileSample\"] == False]\n",
    "combined_memories_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_memories_dataframe[combined_memories_dataframe[\"Model\"] == \"12b\"].value_counts(\"TrainingPercentage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot for data across \n",
    "counts_frame_scale = combined_memories_dataframe[combined_memories_dataframe[\"Checkpoint\"] == \"Final\"].value_counts([\"Param Count\", \"category\"]).unstack().reindex(split_to_param_count.values())\n",
    "counts_frame_scale = counts_frame_scale[categories].dropna()\n",
    "counts_frame_scale.to_csv(f\"final_checkpoint_counts_recitation={RECITATION_THRESHOLD}.csv\")\n",
    "display(counts_frame_scale)\n",
    "\n",
    "intermediate_frame = combined_memories_dataframe[combined_memories_dataframe[\"Model\"] == \"12b\"]\n",
    "sorted_checkpoints = sorted(sorted(intermediate_frame[\"TrainingPercentage\"].unique(), key=lambda x: int(x)))\n",
    "\n",
    "counts_frame_time = intermediate_frame.value_counts([\"TrainingPercentage\", \"category\"]).unstack().reindex(sorted_checkpoints)\n",
    "counts_frame_time = counts_frame_time[categories].dropna()\n",
    "counts_frame_time.to_csv(f\"intermediate_checkpoint_counts_recitation={RECITATION_THRESHOLD}.csv\")\n",
    "display(counts_frame_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Percents and Memories by Taxonomy Across Time and Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_frame = combined_memories_dataframe.value_counts([\"Param Count\", \"category\"]).unstack().reindex(split_to_param_count.values())[categories].dropna()\n",
    "display(counts_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_percents_time = []\n",
    "for checkpoint in tqdm(sorted_checkpoints):\n",
    "    model_examples = combined_memories_dataframe[combined_memories_dataframe[\"TrainingPercentage\"] == checkpoint]\n",
    "    model_percents = model_examples.value_counts(\"category\", normalize=True).to_dict()\n",
    "    for category in model_percents:\n",
    "        all_percents_time.append({\n",
    "            \"TrainingPercentage\": checkpoint,\n",
    "            \"category\": category,\n",
    "            \"percent\": model_percents[category],\n",
    "        })\n",
    "\n",
    "percents_frame_time = pd.DataFrame(all_percents_time).pivot(index=\"TrainingPercentage\", columns=\"category\", values=\"percent\").reindex(sorted_checkpoints)\n",
    "percents_frame_time.index = [f\"{int(percent * 100)}%\" for percent in percents_frame_time.index]\n",
    "percents_frame_time = percents_frame_time[categories]\n",
    "percents_frame_time.to_csv(f\"percents_frame_time_recitation={RECITATION_THRESHOLD}.csv\")\n",
    "display(percents_frame_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_percents_scale = []\n",
    "for param_count in tqdm(split_to_param_count.values()):\n",
    "    model_examples = combined_memories_dataframe[combined_memories_dataframe[\"Param Count\"] == param_count]\n",
    "    if len(model_examples) == 0:\n",
    "        continue\n",
    "\n",
    "    model_percents = model_examples.value_counts(\"category\", normalize=True).to_dict()\n",
    "    for category in categories:\n",
    "        all_percents_scale.append({\n",
    "            \"Model\": model_examples[\"Model\"].unique()[0],\n",
    "            \"Param Count\": param_count,\n",
    "            \"category\": category,\n",
    "            \"percent\": model_percents[category],\n",
    "        })\n",
    "\n",
    "# where Model != 160m\n",
    "model_keys = [key for key in split_to_param_count.keys() if key != \"160m\"]\n",
    "percents_frame_scale = pd.DataFrame(all_percents_scale).pivot(index=\"Model\", columns=\"category\", values=\"percent\").reindex(model_keys)\n",
    "# change column order\n",
    "percents_frame_scale = percents_frame_scale[categories]\n",
    "percents_frame_scale.to_csv(f\"percents_frame_scale_recitation={RECITATION_THRESHOLD}.csv\")\n",
    "display(percents_frame_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure: Combined Counts + Percents Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same chart but with bounded bar charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single row with four plots. Plot ordering is counts across scale, percents across scale, counts across time, percents across time\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "percents_figure_y_lim = (0.75, 1.01)\n",
    "percents_figure_y_ticks = [0.8, 0.9, 1]\n",
    "\n",
    "# set figure 1\n",
    "display(counts_frame_scale)\n",
    "sns.lineplot(ax=axes[0], data=counts_frame_scale, dashes=False, markers=True, markersize=8)\n",
    "axes[0].tick_params(axis='x', rotation=20, labelsize=10)\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].set_xscale(\"log\")\n",
    "axes[0].tick_params(axis='y', labelsize=10)\n",
    "axes[0].set_ylabel(\"Memories\")\n",
    "axes[0].set_xlabel(\"Parameters\")\n",
    "axes[0].legend(loc='upper center', bbox_to_anchor=(2.25, -0.3), ncol=4, frameon=False)\n",
    "\n",
    "# set figure 2\n",
    "display(percents_frame_scale)\n",
    "percents_frame_scale.plot.bar(\n",
    "    stacked=True,\n",
    "    ax=axes[1],\n",
    "    rot=0,\n",
    "    width=1,\n",
    ")\n",
    "axes[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}%\".format(int(x * 100))))\n",
    "axes[1].set_ylim(percents_figure_y_lim)\n",
    "axes[1].set_yticks(percents_figure_y_ticks)\n",
    "axes[1].tick_params(axis='x', rotation=20, labelsize=10)\n",
    "axes[1].tick_params(axis='y', labelsize=10)\n",
    "axes[1].set_xlabel(\"Parameters\")\n",
    "axes[1].get_legend().remove()\n",
    "\n",
    "# Add ...\n",
    "trans = transforms.blended_transform_factory(axes[1].transData, axes[1].transAxes)\n",
    "axes[1].text(-1.5, 0.16, \".\", fontsize=24, transform=trans, ha='center', va='top')\n",
    "axes[1].text(-1.5, 0.13, \".\", fontsize=24, transform=trans, ha='center', va='top')\n",
    "axes[1].text(-1.5, 0.09, \".\", fontsize=24, transform=trans, ha='center', va='top')\n",
    "\n",
    "# set figure 3\n",
    "display(counts_frame_time)\n",
    "sns.lineplot(ax=axes[2], data=counts_frame_time, dashes=False, markers=True, markersize=8)\n",
    "axes[2].set_yscale(\"log\")\n",
    "axes[2].tick_params(axis='x', labelsize=10)\n",
    "axes[2].tick_params(axis='y', labelsize=10)\n",
    "axes[2].tick_params(axis='x', rotation=30)\n",
    "axes[2].set_xlabel(\"Training Time\")\n",
    "axes[2].xaxis.set_major_formatter(PercentFormatter(1))\n",
    "axes[2].legend().remove()\n",
    "\n",
    "# set figure 4\n",
    "display(percents_frame_time)\n",
    "percents_frame_time.plot.bar(\n",
    "    stacked=True,\n",
    "    ax=axes[3],\n",
    "    rot=0,\n",
    "    width=1,\n",
    ")\n",
    "axes[3].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}%\".format(int(x * 100))))\n",
    "axes[3].set_ylim(percents_figure_y_lim)\n",
    "axes[3].set_yticks(percents_figure_y_ticks)\n",
    "axes[3].tick_params(axis='x', rotation=20, labelsize=10)\n",
    "axes[3].tick_params(axis='y', labelsize=10)\n",
    "axes[3].set_xlabel(\"Training Time\")\n",
    "axes[3].get_legend().remove()\n",
    "\n",
    "# Add ...\n",
    "trans = transforms.blended_transform_factory(axes[3].transData, axes[3].transAxes)\n",
    "axes[3].text(-1.5, 0.16, \".\", fontsize=24, transform=trans, ha='center', va='top')\n",
    "axes[3].text(-1.5, 0.13, \".\", fontsize=24, transform=trans, ha='center', va='top')\n",
    "axes[3].text(-1.5, 0.09, \".\", fontsize=24, transform=trans, ha='center', va='top')\n",
    "\n",
    "\n",
    "# make all the x labels have the same height\n",
    "fig.align_xlabels()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig(f\"{figures_path}/categories_counts_percents_across_time+scale_bounded_bars.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code VS NL by Category Across Time and Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_memories_dataframe[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dir = \"./data/nl_vs_code_stats\"\n",
    "if not os.path.exists(stats_dir):\n",
    "    os.makedirs(stats_dir)\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(20, 16))\n",
    "for row_index, row_category in tqdm(enumerate([\"All Memories\", \"Recitation\", \"Reconstruction\", \"Recollection\"]), desc=\"Plotting Categories\", total=4):\n",
    "    row_frame = combined_memories_dataframe[combined_memories_dataframe[\"category\"] == row_category] if row_category != \"All Memories\" else combined_memories_dataframe\n",
    "    row_frame[\"Type\"] = row_frame[\"IsCode\"].apply(lambda x: \"Code\" if x else \"NL\")\n",
    "\n",
    "    # Get the count of NL vs Code across scale\n",
    "    nl_code_counts_frame_scale = row_frame[(row_frame[\"Checkpoint\"] == \"Final\")].value_counts([\"Param Count\", \"Type\"]).unstack().reindex(split_to_param_count.values()).dropna()\n",
    "    nl_code_counts_frame_scale.to_csv(f\"{stats_dir}/{row_category}_counts_scale.csv\")\n",
    "\n",
    "    # Get the percent of NL vs Code across scale\n",
    "    all_percents_scale = []\n",
    "    for param_count in tqdm(split_to_param_count.values(), desc=\"Calculating Percents Across Scale\"):\n",
    "        model_examples = row_frame[row_frame[\"Param Count\"] == param_count]\n",
    "        model_percents = model_examples.value_counts(\"Type\", normalize=True).to_dict()\n",
    "        for category in model_percents:\n",
    "            all_percents_scale.append({\n",
    "                \"Model\": model_examples[\"Model\"].unique()[0],\n",
    "                \"Param Count\": param_count,\n",
    "                \"Type\": category,\n",
    "                \"percent\": model_percents[category],\n",
    "            })\n",
    "    \n",
    "    nl_code_percents_frame_scale = pd.DataFrame(all_percents_scale).pivot(index=\"Model\", columns=\"Type\", values=\"percent\").reindex(model_keys)\n",
    "    nl_code_percents_frame_scale.to_csv(f\"{stats_dir}/{row_category}_percents_scale.csv\")\n",
    "\n",
    "    # set figure 1\n",
    "    sns.lineplot(ax=axes[row_index, 0], data=nl_code_counts_frame_scale, dashes=False, markers=True, markersize=8)\n",
    "    axes[row_index, 0].tick_params(axis='x', rotation=20, labelsize=10)\n",
    "    axes[row_index, 0].set_xscale(\"log\")\n",
    "    axes[row_index, 0].tick_params(axis='y', labelsize=10)\n",
    "    axes[row_index, 0].set_ylabel(row_category)\n",
    "    axes[row_index, 0].set_xlabel(\"Parameters\")\n",
    "\n",
    "    # set figure 2\n",
    "    nl_code_percents_frame_scale.plot.bar(\n",
    "        stacked=True,\n",
    "        ax=axes[row_index, 1],\n",
    "        rot=0,\n",
    "        width=1,\n",
    "    )\n",
    "    axes[row_index, 1].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}%\".format(int(x * 100))))\n",
    "    axes[row_index, 1].tick_params(axis='x', rotation=20, labelsize=10)\n",
    "    axes[row_index, 1].tick_params(axis='y', labelsize=10)\n",
    "\n",
    "    # set figure 3\n",
    "    intermediate_frame = row_frame[row_frame[\"Model\"] == \"12b\"]\n",
    "    sorted_checkpoints = sorted(sorted(intermediate_frame[\"TrainingPercentage\"].unique(), key=lambda x: int(x)))\n",
    "    nl_code_counts_frame_time = intermediate_frame.value_counts([\"TrainingPercentage\", \"Type\"]).unstack().reindex(sorted_checkpoints)\n",
    "    nl_code_counts_frame_time.to_csv(f\"{stats_dir}/{row_category}_counts_time.csv\")\n",
    "    \n",
    "    sns.lineplot(ax=axes[row_index, 2], data=nl_code_counts_frame_time, dashes=False, markers=True, markersize=8)\n",
    "    axes[row_index, 2].tick_params(axis='x', labelsize=10)\n",
    "    axes[row_index, 2].tick_params(axis='y', labelsize=10)\n",
    "    axes[row_index, 2].tick_params(axis='x', rotation=30)\n",
    "    axes[row_index, 2].xaxis.set_major_formatter(PercentFormatter(1))\n",
    "\n",
    "    # set figure 4\n",
    "    all_percents_time = []\n",
    "    for checkpoint in tqdm(sorted_checkpoints):\n",
    "        model_examples = row_frame[(row_frame[\"TrainingPercentage\"] == checkpoint)]\n",
    "        model_percents = model_examples.value_counts(\"Type\", normalize=True).to_dict()\n",
    "        for category in model_percents:\n",
    "            all_percents_time.append({\n",
    "                \"TrainingPercentage\": checkpoint,\n",
    "                \"Type\": category,\n",
    "                \"percent\": model_percents[category],\n",
    "            })\n",
    "        \n",
    "    nl_code_percents_frame_time = pd.DataFrame(all_percents_time).pivot(index=\"TrainingPercentage\", columns=\"Type\", values=\"percent\").reindex(sorted_checkpoints)\n",
    "    nl_code_percents_frame_time.to_csv(f\"{stats_dir}/{row_category}_percents_time.csv\")\n",
    "    nl_code_percents_frame_time.index = [f\"{int(percent * 100)}%\" for percent in nl_code_percents_frame_time.index]\n",
    "    \n",
    "    nl_code_percents_frame_time.plot.bar(\n",
    "        stacked=True,\n",
    "        ax=axes[row_index, 3],\n",
    "        rot=0,\n",
    "        width=1,\n",
    "    )\n",
    "    axes[row_index, 3].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}%\".format(int(x * 100))))\n",
    "    axes[row_index, 3].tick_params(axis='x', rotation=20, labelsize=10)\n",
    "    axes[row_index, 3].tick_params(axis='y', labelsize=10)\n",
    "\n",
    "    # Set x labels for last row\n",
    "    if row_index == 3:\n",
    "        axes[row_index, 0].set_xlabel(\"Parameters\")\n",
    "        axes[row_index, 1].set_xlabel(\"Parameters\")\n",
    "        axes[row_index, 2].set_xlabel(\"Training Time\")\n",
    "        axes[row_index, 3].set_xlabel(\"Training Time\")\n",
    "    else:\n",
    "        axes[row_index, 0].set_xlabel(\"\")\n",
    "        axes[row_index, 1].set_xlabel(\"\")\n",
    "        axes[row_index, 2].set_xlabel(\"\")\n",
    "        axes[row_index, 3].set_xlabel(\"\")\n",
    "    \n",
    "    if row_index == 3:\n",
    "        axes[row_index, 0].legend(loc='upper center', bbox_to_anchor=(2.25, -0.3), ncol=4, frameon=False, fontsize=24)\n",
    "    else:\n",
    "        axes[row_index, 0].get_legend().remove()\n",
    "    \n",
    "    axes[row_index, 1].get_legend().remove() if axes[row_index, 1].get_legend() else None\n",
    "    axes[row_index, 2].get_legend().remove() if axes[row_index, 2].get_legend() else None\n",
    "    axes[row_index, 3].get_legend().remove() if axes[row_index, 3].get_legend() else None\n",
    "\n",
    "fig.align_xlabels()\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.25, wspace=0.25)\n",
    "fig.savefig(f\"{figures_path}/nl_code_counts_percents_across_time+scale.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LaTEX Table for Code Proportion by Category Across Time and Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_counts_frame = None\n",
    "combined_percents_frame = None\n",
    "\n",
    "for file_name in os.listdir(stats_dir):\n",
    "    category = file_name.split(\"_\")[0]\n",
    "    metric = file_name.split(\"_\")[1].split(\".\")[0]\n",
    "    if \"counts\" in metric:\n",
    "        continue\n",
    "\n",
    "    current_frame = pd.read_csv(f\"{stats_dir}/{file_name}\")\n",
    "    current_frame[\"Category\"] = category\n",
    "    current_frame[\"Metric\"] = metric\n",
    "\n",
    "    if \"TrainingPercentage\" not in current_frame.columns:\n",
    "        current_frame[\"TrainingPercentage\"] = 1\n",
    "    \n",
    "    if \"Param Count\" not in current_frame.columns:\n",
    "        current_frame[\"Param Count\"] = 12000000000\n",
    "    \n",
    "    if \"Unnamed: 0\" in current_frame.columns:\n",
    "        current_frame.rename(columns={\"Unnamed: 0\": \"TrainingPercentage\"}, inplace=True)\n",
    "\n",
    "    if \"Model\" not in current_frame.columns:\n",
    "        # swap keys and values in split_to_param_count\n",
    "        param_count_to_split = {value: key for key, value in split_to_param_count.items()}\n",
    "        current_frame[\"Model\"] = current_frame[\"Param Count\"].apply(lambda x: param_count_to_split[x])\n",
    "\n",
    "    if combined_percents_frame is None:\n",
    "        combined_percents_frame = current_frame\n",
    "    else:\n",
    "        combined_percents_frame = pd.concat([combined_percents_frame, current_frame])\n",
    "\n",
    "# display(combined_counts_frame.head())\n",
    "# display(combined_percents_frame.head())\n",
    "\n",
    "final_checkpoints_nl_code_percents = combined_percents_frame[combined_percents_frame[\"TrainingPercentage\"] == 1].sort_values(\"Param Count\")\n",
    "\n",
    "# Across Scale\n",
    "for model_size in list(split_to_param_count.keys()):\n",
    "    if model_size == \"160m\":\n",
    "        continue\n",
    "\n",
    "    model_frame = final_checkpoints_nl_code_percents[final_checkpoints_nl_code_percents[\"Model\"] == model_size]\n",
    "    all_memories_row = model_frame[model_frame[\"Category\"] == \"All Memories\"]\n",
    "    recitation_row = model_frame[model_frame[\"Category\"] == \"Recitation\"]\n",
    "    reconstruction_row = model_frame[model_frame[\"Category\"] == \"Reconstruction\"]\n",
    "    recollection_row = model_frame[model_frame[\"Category\"] == \"Recollection\"]\n",
    "\n",
    "    latex_row = f\"{model_size} & {all_memories_row['Code'].iloc[0]:.2%} & {all_memories_row['NL'].iloc[0]:.2%} & {recitation_row['Code'].iloc[0]:.2%} & {recitation_row['NL'].iloc[0]:.2%} & {reconstruction_row['Code'].iloc[0]:.2%} & {reconstruction_row['NL'].iloc[0]:.2%} & {recollection_row['Code'].iloc[0]:.2%} & {recollection_row['NL'].iloc[0]:.2%} \\\\\\\\\"\n",
    "    latex_row = latex_row.replace(\"%\", \"\\%\")\n",
    "    print(latex_row)\n",
    "\n",
    "# Across Time\n",
    "intermediate_frame_nl_code_percents = combined_percents_frame[combined_percents_frame[\"Model\"] == \"12b\"]\n",
    "intermediate_frame_nl_code_percents.sort_values(\"TrainingPercentage\", inplace=True)\n",
    "intermediate_frame_nl_code_percents[\"TrainingPercentage\"] = intermediate_frame_nl_code_percents[\"TrainingPercentage\"].astype(str)\n",
    "unqiue_checkpoints = intermediate_frame_nl_code_percents[\"TrainingPercentage\"].unique()\n",
    "display(intermediate_frame_nl_code_percents)\n",
    "display(sorted_training_percentages)\n",
    "for checkpoint in unqiue_checkpoints:\n",
    "    model_frame = intermediate_frame_nl_code_percents[intermediate_frame_nl_code_percents[\"TrainingPercentage\"] == checkpoint]\n",
    "    all_memories_row = model_frame[model_frame[\"Category\"] == \"All Memories\"]\n",
    "    recitation_row = model_frame[model_frame[\"Category\"] == \"Recitation\"]\n",
    "    reconstruction_row = model_frame[model_frame[\"Category\"] == \"Reconstruction\"]\n",
    "    recollection_row = model_frame[model_frame[\"Category\"] == \"Recollection\"]\n",
    "\n",
    "    # print(checkpoint)\n",
    "    # display(model_frame)\n",
    "    # display(all_memories_row)\n",
    "    # display(recitation_row)\n",
    "    # display(reconstruction_row)\n",
    "    # display(recollection_row)\n",
    "\n",
    "    latex_row = f\"{int(float(checkpoint) * 100)}% & {all_memories_row['Code'].iloc[0]:.2%} & {all_memories_row['NL'].iloc[0]:.2%} & {recitation_row['Code'].iloc[0]:.2%} & {recitation_row['NL'].iloc[0]:.2%} & {reconstruction_row['Code'].iloc[0]:.2%} & {reconstruction_row['NL'].iloc[0]:.2%} & {recollection_row['Code'].iloc[0]:.2%} & {recollection_row['NL'].iloc[0]:.2%} \\\\\\\\\"\n",
    "    latex_row = latex_row.replace(\"%\", \"\\%\")\n",
    "    print(latex_row)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12b deduped final checkpoint\n",
    "hists_plotting_frame = combined_dataframe[(combined_dataframe[\"Model\"] == \"12b\") & (combined_dataframe[\"Checkpoint\"] == \"Final\") & (combined_dataframe[\"Deduped\"] == True)]\n",
    "hists_plotting_frame = hists_plotting_frame.drop_duplicates(subset=[\"sequence_id\"])\n",
    "hists_plotting_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_splits = [\"_deduped_12b\"]\n",
    "features = [\n",
    "    \"sequence_duplicates\",\n",
    "    \"max_frequency\",\n",
    "    \"avg_frequency\",\n",
    "    \"min_frequency\",\n",
    "    \"median_frequency\",\n",
    "    \"p25_frequency\", \"p75_frequency\",\n",
    "    \"0_8_templates\", \"huffman_coding_length\", \"prompt_perplexity\", \"generation_perplexity\", \"sequence_perplexity\", \"0_8_snowclones\", \"loss\"\n",
    "]\n",
    "bins_per_feature = {\n",
    "    \"sequence_duplicates\": 10,\n",
    "    \"max_frequency\": 15,\n",
    "    \"avg_frequency\": 150,\n",
    "    \"min_frequency\": 150,\n",
    "    \"median_frequency\": 100,\n",
    "    \"p25_frequency\": 100,\n",
    "    \"p75_frequency\": 20,\n",
    "    \"0_8_templates\": 20,\n",
    "    \"prompt_perplexity\": 100,\n",
    "    \"generation_perplexity\": 35,\n",
    "    \"sequence_perplexity\": 100, \n",
    "    \"huffman_coding_length\": 60, #dont take log scale for this feature \n",
    "    \"0_8_snowclones\": 60, \n",
    "    \"loss\": 50 \n",
    "}\n",
    "min_threshold = {\n",
    "    \"sequence_duplicates\": 10**0,\n",
    "    \"max_frequency\": 10**8.5,\n",
    "    \"avg_frequency\": 10**8,\n",
    "    \"min_frequency\": 10**4.5,\n",
    "    \"median_frequency\": 10**6,\n",
    "    \"p25_frequency\": 10**5,\n",
    "    \"p75_frequency\": 10**6,\n",
    "    \"prompt_perplexity\": 10**0,\n",
    "    \"generation_perplexity\": 10**0,\n",
    "    \"sequence_perplexity\": 10**0, \n",
    "    \"loss\": 10**-0.5,\n",
    "    \"huffman_coding_length\": 2, \n",
    "    \"0_8_templates\": 10**-0.6,\n",
    "    \"0_8_snowclones\": 10**0, \n",
    "}\n",
    "max_threshold = {\n",
    "    \"sequence_duplicates\": 10**7,\n",
    "    \"max_frequency\": 10**10,\n",
    "    \"avg_frequency\": 10**10,\n",
    "    \"min_frequency\": 10**8,\n",
    "    \"median_frequency\": 10**10,\n",
    "    \"p25_frequency\": 10**9,\n",
    "    \"p75_frequency\": 10**10,\n",
    "    \"0_8_templates\": 10**4,\n",
    "    \"prompt_perplexity\": 10**1.5,\n",
    "    \"generation_perplexity\": 10**1.1,\n",
    "    \"sequence_perplexity\": 10**2.5, \n",
    "    \"huffman_coding_length\": 6,\n",
    "    \"0_8_snowclones\": 10**3.1, \n",
    "    \"loss\": 10**0.5\n",
    "}\n",
    "name_map = {\n",
    "    \"sequence_duplicates\": \"Duplicates\",\n",
    "    \"0_8_templates\": \"Textual Duplicates\",\n",
    "    \"0_8_snowclones\": \"Semantic Duplicates\",\n",
    "    \"prompt_perplexity\": \"Prompt PPL\",\n",
    "    \"generation_perplexity\": \"Generation PPL\",\n",
    "    \"sequence_perplexity\": \"Sequence PPL\",\n",
    "    \"loss\": \"Loss\",\n",
    "    \"max_frequency\": \"Max Token Freq.\",\n",
    "    \"avg_frequency\": \"Mean Token Freq.\",\n",
    "    \"min_frequency\": \"Min Token Freq.\",\n",
    "    \"median_frequency\": \"Median Token Freq.\",\n",
    "    \"p25_frequency\": \"P25 Token Freq.\",\n",
    "    \"p75_frequency\": \"P75 Token Freq.\",\n",
    "    \"huffman_coding_length\": \"Huffman Length\",\n",
    "}\n",
    "e = 1e-10\n",
    "num_rows = 2\n",
    "num_columns = 7 \n",
    "\n",
    "fig, axs = plt.subplots(num_rows, num_columns, figsize=(17, 5))\n",
    "axs = axs.flatten()\n",
    "for i, split in enumerate(hist_splits):\n",
    "    for j, fx in tqdm(enumerate(name_map.keys()), desc=\"Plotting Features\", total=len(features)):\n",
    "        memories = hists_plotting_frame[hists_plotting_frame[\"Memorized\"] == True][fx]\n",
    "        memories = [value for value in memories if value >= 0]\n",
    "        df_memories = pd.DataFrame(memories, columns=[fx])\n",
    "\n",
    "        ppile = hists_plotting_frame[hists_plotting_frame[\"Memorized\"] == False][fx]\n",
    "        ppile = [value for value in ppile if value >= 0]\n",
    "        df_pile = pd.DataFrame(ppile, columns=[fx])\n",
    "\n",
    "    \n",
    "        bins_all = np.logspace(np.log10(min(df_memories[fx].min(), df_pile[fx].min())+e), np.log10(max(df_memories[fx].max(), df_pile[fx].max())), bins_per_feature[fx])\n",
    "        if fx == \"huffman_coding_length\":\n",
    "            bins = 60 \n",
    "        else: \n",
    "            bins = bins_all  \n",
    "\n",
    "        # no whitespace between histograms for continuous features. Make width a bit larger\n",
    "        sns.histplot(data=df_pile[fx], bins=bins, label=\"Pile\", ax=axs[i * num_columns + j], stat=\"percent\", element=\"step\")\n",
    "        sns.histplot(data=df_memories[fx], bins=bins, label=\"Memorized\", ax=axs[i * num_columns + j], stat=\"percent\", element=\"step\")\n",
    "\n",
    "        if fx == \"huffman_coding_length\":\n",
    "            axs[i * num_columns + j].set_xscale(\"linear\") \n",
    "            axs[i * num_columns + j].yaxis.set_major_formatter(PercentFormatter(xmax=100, decimals=0))\n",
    "            axs[i * num_columns + j].set_xlim(min_threshold[fx], max_threshold[fx])\n",
    "            axs[i * num_columns + j].xaxis.set_major_locator(MaxNLocator(nbins=3))\n",
    "            axs[i * num_columns + j].yaxis.set_major_locator(MaxNLocator(nbins=3))\n",
    "\n",
    "        elif fx == \"loss\":\n",
    "            axs[i * num_columns + j].set_xscale(\"log\")   \n",
    "            axs[i * num_columns + j].yaxis.set_major_formatter(PercentFormatter(xmax=100, decimals=0))\n",
    "            axs[i * num_columns + j].xaxis.set_major_locator(LogLocator(numticks=3)) \n",
    "            axs[i * num_columns + j].yaxis.set_major_locator(MaxNLocator(nbins=3))\n",
    "           \n",
    "        else:\n",
    "            axs[i * num_columns + j].set_xscale(\"log\")   \n",
    "            axs[i * num_columns + j].yaxis.set_major_formatter(PercentFormatter(xmax=100, decimals=0))\n",
    "            axs[i * num_columns + j].set_xlim(min_threshold[fx], max_threshold[fx])\n",
    "            axs[i * num_columns + j].xaxis.set_major_locator(LogLocator(numticks=3)) \n",
    "            axs[i * num_columns + j].yaxis.set_major_locator(MaxNLocator(nbins=3))\n",
    "         \n",
    "        axs[i * num_columns + j].tick_params(axis=\"both\", labelsize=14) \n",
    "        axs[i * num_columns + j].set_xlabel(name_map[fx], fontsize=16)\n",
    "        if j % num_columns == 0:\n",
    "            axs[i * num_columns + j].set_ylabel(\"Percentage\", fontsize=16)\n",
    "        else:\n",
    "            axs[i * num_columns + j].set_ylabel(\"\")\n",
    "\n",
    "fig.legend(labels=[\"Not Memorized\", \"Memorized\"], loc=\"upper center\", bbox_to_anchor=(0.5, -0.005), ncol=2, fontsize=18, frameon=False)\n",
    "fig.align_xlabels()\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"{figures_path}/histograms_percents.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memorization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

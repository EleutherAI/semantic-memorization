{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchorse/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, GPTNeoXForCausalLM, GPTNeoXConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFMemoriesDataset(Dataset):\n",
    "    is_dataframe = False\n",
    "\n",
    "    def __init__(self, memories, tokenizer, sample=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.memories = memories\n",
    "        if sample is not None:\n",
    "            self.memories = self.memories.to_pandas().sample(sample)\n",
    "            self.is_dataframe = True\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        memory_record = (\n",
    "            self.memories.iloc[index] if self.is_dataframe else self.memories[index]\n",
    "        )\n",
    "        decoded_text = self.tokenizer.decode(memory_record[\"tokens\"])\n",
    "        return decoded_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memories)\n",
    "\n",
    "\n",
    "def load_tokenizer(split_name):\n",
    "    isDeduped = split_name.startswith(\"deduped\")\n",
    "    model = split_name.split(\"duped.\")[-1]\n",
    "    corresponding_model = f\"EleutherAI/pythia-{model}{'-deduped' if isDeduped else ''}\"\n",
    "    tokenizer =  AutoTokenizer.from_pretrained(corresponding_model)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "def load_model(split_name):\n",
    "    isDeduped = split_name.startswith(\"deduped\")\n",
    "    model = split_name.split(\"duped.\")[-1]\n",
    "    corresponding_model = f\"EleutherAI/pythia-{model}{'-deduped' if isDeduped else ''}\"\n",
    "    device = torch.device(\"cuda:7\")\n",
    "    return GPTNeoXForCausalLM.from_pretrained(corresponding_model, device_map=\"auto\", offload_folder=\"offload\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/mchorse/.cache/huggingface/datasets/EleutherAI___parquet/EleutherAI--pythia-memorized-evals-623aaa371a33821a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 16/16 [00:00<00:00, 31.29it/s]\n",
      "Found cached dataset parquet (/home/mchorse/.cache/huggingface/datasets/EleutherAI___parquet/EleutherAI--pythia-memorized-evals-623aaa371a33821a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 16/16 [00:00<00:00, 39.55it/s]\n"
     ]
    }
   ],
   "source": [
    "split_name = \"deduped.12b\"\n",
    "memories = load_dataset(\"EleutherAI/pythia-memorized-evals\")[split_name]\n",
    "tokenizer = load_tokenizer(split_name)\n",
    "memories_dataset = HFMemoriesDataset(\n",
    "    load_dataset(\"EleutherAI/pythia-memorized-evals\")[split_name], \n",
    "    tokenizer)\n",
    "\n",
    "pythia_model = load_model(split_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.78515625]\n",
      "[4.78515625]\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(logits, labels):\n",
    "    # Store the probabilities for each token. These will be summed later, but having the \n",
    "    # individual probabilities is helpful for debugging.\n",
    "    token_probs = []\n",
    "\n",
    "    # Don't include the final token logits. There are no labels for these since the sequence has ended.\n",
    "    shifted_logits = logits.detach()[:-1, :]\n",
    "\n",
    "    for token_index in range(len(shifted_logits)):\n",
    "        # Map the logits to probabilities.\n",
    "        predicted_probs = torch.softmax(shifted_logits[token_index], dim=0)\n",
    "        # Get the probability of the correct label.\n",
    "        label_prob = predicted_probs[labels[token_index + 1]]\n",
    "        # Store the probability for this token.\n",
    "        token_probs.append(label_prob.detach())\n",
    "    \n",
    "    # Caluclate the log-likelyhood of the sequence by summing the probabilities of each token and then taking the log.\n",
    "    log_likelihood = torch.log(torch.stack(token_probs)).sum()\n",
    "    \n",
    "    # Caluclate the cross entropy by dividing the negative log-likelihood by the number of tokens.\n",
    "    cross_entropy = -log_likelihood / len(shifted_logits)\n",
    "\n",
    "    # Calculate the perplexity by taking the exponential of the cross entropy.\n",
    "    perplexity = torch.exp(cross_entropy).item()\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "data_loader = DataLoader(memories_dataset, batch_size=32)\n",
    "hf_perplexities = []\n",
    "all_perplexities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in [memories_dataset[0]]:\n",
    "        tokenized_batch = tokenizer(\n",
    "            batch, return_tensors=\"pt\", max_length=512, truncation=True, padding=True\n",
    "        )\n",
    "        tokenized_batch.to(torch.device(\"cuda:7\"))\n",
    "        labels = tokenized_batch[\"input_ids\"]\n",
    "\n",
    "        outputs = pythia_model(**tokenized_batch, labels=labels)\n",
    "        logits = outputs.logits.detach()\n",
    "        hf_perplexities += [torch.exp(outputs.loss).item()]\n",
    "\n",
    "        all_perplexities += [calculate_perplexity(logits[i], labels[i]) for i in range(len(logits))]\n",
    "\n",
    "print(hf_perplexities)\n",
    "print(all_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = memories_dataset[0]\n",
    "display(sequence)\n",
    "\n",
    "tokenized_batch = tokenizer(sequence, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "tokenized_batch[\"attention_mask\"][0][1:] = 0\n",
    "tokenized_batch.to(torch.device(\"cuda:7\"))\n",
    "labels = tokenized_batch[\"input_ids\"]\n",
    "\n",
    "likelihoods = None\n",
    "with torch.no_grad():\n",
    "    for i in range(2, len(labels[0])):\n",
    "        # tokenized_batch[\"attention_mask\"][0][:i] = 1\n",
    "        outputs = pythia_model(**tokenized_batch, labels=labels)\n",
    "        probs = torch.softmax(outputs.logits, dim=0)\n",
    "        # likelyhood = outputs.logits[0][labels[0][i]]\n",
    "        loss = outputs.loss * i\n",
    "        likelihoods = loss if likelihoods is None else likelihoods + outputs.loss\n",
    "\n",
    "log_likelihoods = torch.log(likelihoods)\n",
    "perplexity = torch.exp(-log_likelihoods / len(labels[0])).item()\n",
    "perplexity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memories_df = memories.to_pandas()[[\"index\"]]\n",
    "memories_df[\"perplexity\"] = all_perplexities\n",
    "memories_df[[\"index\", \"perplexity\"]].to_csv(f\"./datasets/memories_{split_name}.csv\", index=False)\n",
    "memories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_sequence = memories_dataset[0]\n",
    "    perplexity = pipeline(\"text-generation\", model=pythia_model, tokenizer=tokenizer, device=0, metric=\"perplexity\")\n",
    "    perplexity_score = perplexity(test_sequence)[0][\"score\"]\n",
    "    perplexity_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kyle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

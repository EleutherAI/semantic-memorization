{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPTNeoXForCausalLM, GPTNeoXConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFMemoriesDataset(Dataset):\n",
    "    is_dataframe = False\n",
    "\n",
    "    def __init__(self, memories, tokenizer, sample=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.memories = memories\n",
    "        if sample is not None:\n",
    "            self.memories = self.memories.to_pandas().sample(sample)\n",
    "            self.is_dataframe = True\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        memory_record = (\n",
    "            self.memories.iloc[index] if self.is_dataframe else self.memories[index]\n",
    "        )\n",
    "        decoded_text = self.tokenizer.decode(memory_record[\"tokens\"])\n",
    "        return decoded_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memories)\n",
    "\n",
    "\n",
    "def load_tokenizer(split_name):\n",
    "    isDeduped = split_name.startswith(\"deduped\")\n",
    "    model = split_name.split(\"duped.\")[-1]\n",
    "    corresponding_model = f\"EleutherAI/pythia-{model}{'-deduped' if isDeduped else ''}\"\n",
    "    tokenizer =  AutoTokenizer.from_pretrained(corresponding_model)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "def load_model(split_name):\n",
    "    isDeduped = split_name.startswith(\"deduped\")\n",
    "    model = split_name.split(\"duped.\")[-1]\n",
    "    corresponding_model = f\"EleutherAI/pythia-{model}{'-deduped' if isDeduped else ''}\"\n",
    "    device = torch.device(\"cuda:7\")\n",
    "    return GPTNeoXForCausalLM.from_pretrained(corresponding_model, device_map=\"auto\", offload_folder=\"offload\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_name = \"deduped.12b\"\n",
    "memories = load_dataset(\"EleutherAI/pythia-memorized-evals\")[split_name]\n",
    "tokenizer = load_tokenizer(split_name)\n",
    "memories_dataset = HFMemoriesDataset(\n",
    "    load_dataset(\"EleutherAI/pythia-memorized-evals\")[split_name], \n",
    "    tokenizer)\n",
    "\n",
    "pythia_model = load_model(split_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(logits, labels):\n",
    "    shift_logits = logits.detach()[:-1, :].contiguous()\n",
    "    shift_labels = labels[1:].contiguous()\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    cross_entropy = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    perplexity = torch.exp(cross_entropy)\n",
    "    return perplexity\n",
    "\n",
    "data_loader = DataLoader(memories_dataset, batch_size=32)\n",
    "all_perplexities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(data_loader):\n",
    "        tokenized_batch = tokenizer(\n",
    "            batch, return_tensors=\"pt\", max_length=512, truncation=True, padding=True\n",
    "        )\n",
    "        tokenized_batch.to(torch.device(\"cuda:7\"))\n",
    "        labels = tokenized_batch[\"input_ids\"][:, 1:].contiguous()\n",
    "\n",
    "        outputs = pythia_model(**tokenized_batch, labels=tokenized_batch[\"input_ids\"])\n",
    "        logits = outputs.logits.detach()\n",
    "\n",
    "        labels = tokenized_batch[\"input_ids\"]\n",
    "        perplexities = [calculate_perplexity(logits[i], labels[i]) for i in range(len(logits))]\n",
    "        all_perplexities += [perplexity.item() for perplexity in perplexities]\n",
    "\n",
    "print(len(all_perplexities))\n",
    "all_perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memories_df = memories.to_pandas()[[\"index\"]]\n",
    "memories_df[\"perplexity\"] = all_perplexities\n",
    "memories_df[[\"index\", \"perplexity\"]].to_csv(f\"./datasets/memories_{split_name}.csv\", index=False)\n",
    "memories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kyle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
